# AST Normalization Architecture

**Purpose**: Transform TypeScript AST into clean, analyzable forms that enable Lobster-style ownership analysis

**Goal**: Enable **85-90% RC elimination** (validated by research)

**Phase**: Week 5-6 (Month 1-2)

---

## ðŸ”´ STATUS: FRAMEWORK ONLY - TRANSFORMATIONS NOT YET IMPLEMENTED

**What Works:**
- âœ… Normalization framework (329 lines) - `src/macro/normalize.zig`
- âœ… Pipeline integration (Phase 2.5 in `src/cli/compile.zig`)
- âœ… Stats tracking and observability
- âœ… `--no-normalize` CLI flag for benchmarking
- âœ… Compilation overhead measured: **3.28%** (target: <5%) âœ“

**What Doesn't Work Yet:**
- âŒ Pass 1: Object spread normalization (TODO)
- âŒ Pass 2: Array method chain fusion (TODO)
- âŒ Pass 3: Closure inlining (TODO)
- âŒ Type checker integration (needed for Pass 1)
- âŒ Parser support for spread elements

**Current RC Elimination Rate:** **0%** (transformations are placeholders)

**Expected Timeline to Full Implementation:**
- Phase 3-4: Type integration + Parser extensions (4 days)
- Phase 5: Pass 1 implementation (3 days)
- Phase 6-7: Validation (4 days)
- **Total:** ~2-3 weeks to 30-50% RC elimination with Pass 1

---

## The Problem

**Traditional TypeScript compilation**:
```
TypeScript Source â†’ AST â†’ Type Check â†’ IR â†’ Codegen
                            â†‘
                     Sees dynamic patterns
                     Marks as SHARED (conservative)
                     Result: 20-30% RC elimination
```

**Problem patterns**:
- Object spread (`{...obj}`) - unknown fields at analysis time
- Array methods (`.map().filter()`) - intermediate allocations
- Closures - implicit capture, unclear ownership
- Dynamic property access - conservative analysis

**Result**: Most objects marked as SHARED â†’ high RC overhead

---

## The Solution: Two-Layer Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Normalization (This Document)                â”‚
â”‚  TypeScript â†’ Clean AST (explicit ownership)            â”‚
â”‚  Result: 70-90% of patterns simplified                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Lobster Ownership Analysis (Week 7-8)        â”‚
â”‚  Clean AST â†’ wants/provides matching                    â”‚
â”‚  Result: 85-90% RC elimination âœ…                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Compilation Pipeline Integration

### Current Pipeline (Before Normalization)

```
Lexer â†’ Parser â†’ Type Checker â†’ IR Generator â†’ Codegen
  â†“       â†“          â†“              â†“             â†“
tokens   AST      typed AST      IR nodes     C/JS/Erlang
```

### New Pipeline (With Normalization)

```
Lexer â†’ Parser â†’ **Normalize** â†’ Type Checker â†’ Ownership Analysis â†’ IR â†’ Codegen
  â†“       â†“           â†“               â†“                â†“            â†“       â†“
tokens   AST   clean AST       typed AST      wants/provides    IR nodes  C/JS
```

**Key insight**: Normalization happens **AFTER parsing** but **BEFORE type checking**

**Why this order?**:
1. Parser produces raw AST (all patterns valid)
2. Normalization simplifies patterns (type-agnostic)
3. Type checker sees clean AST (easier to analyze)
4. Ownership analysis sees explicit ownership (Lobster-style)

---

## The Three Normalization Passes

### Pass 1: Object Spread Normalization

**Input** (dynamic, hard to analyze):
```typescript
const merged = { ...obj1, ...obj2, extra: value };
```

**Output** (explicit ownership):
```typescript
const merged = {
  field1: obj1.field1,  // BORROWED from obj1
  field2: obj1.field2,  // BORROWED from obj1
  field3: obj2.field3,  // BORROWED from obj2
  field4: obj2.field4,  // BORROWED from obj2
  extra: value          // MOVED (last use detected)
};
```

**Ownership Analysis**:
- `merged`: OWNED (new allocation)
- `obj1`, `obj2`: BORROWED (no RC)
- `value`: MOVED if last use (no RC)

**Result**: 0 RC operations (100% elimination!)

**Impact**: **Critical** - Object spreads are common in TypeScript

---

### Pass 2: Array Method Chain Fusion

**Input** (intermediate allocations):
```typescript
const result = items
  .map(x => x * 2)
  .filter(x => x > 10)
  .reduce((a, b) => a + b, 0);
```

**Traditional execution**:
- Allocate array for `.map()` result
- Allocate array for `.filter()` result
- Reduce to final value
- **Cost**: 2 allocations + ~15 RC operations

**Output** (single loop, no intermediates):
```typescript
var result = 0;
for (let i = 0; i < items.length; i++) {
    const x = items[i];      // BORROWED
    const doubled = x * 2;   // Value type (no RC)
    if (doubled > 10) {
        result += doubled;   // Value type (no RC)
    }
}
```

**Ownership Analysis**:
- `items`: BORROWED (no RC)
- `x`, `doubled`: Value types (no RC)
- `result`: Local variable (no RC)

**Result**: 0 RC operations, 0 allocations (vs 2 allocations + 15 RC ops)

**Impact**: **High** - Eliminates most array chain overhead

---

### Pass 3: Closure Inlining

**Input** (implicit capture):
```typescript
function makeCounter(start: number) {
    return () => start++;  // Captures 'start'
}
```

**Traditional execution**:
- Create hidden closure object
- Unclear: is `start` moved or copied?
- Conservative: mark as SHARED

**Output** (explicit context):
```typescript
// Generated closure context struct
struct Counter_Context {
    start: number;  // OWNED by closure
}

function makeCounter(start: number) {
    const ctx = new Counter_Context();  // OWNED
    ctx.start = start;                  // MOVE (last use)

    function Counter_closure(ctx: lent Counter_Context) {
        return ctx.start++;  // BORROWED ctx
    }

    return Counter_closure.bind(ctx);  // MOVE ctx ownership
}
```

**Ownership Analysis**:
- `ctx`: OWNED by closure
- `ctx` in `Counter_closure`: BORROWED (no RC)
- Return: MOVE (no RC)

**Result**: 1 allocation (inevitable), 0 RC operations

**Impact**: **Medium** - Closures less common than spreads/arrays

---

## Implementation Plan

### Week 5: Pass 1 - Object Spread Normalization

**File**: `src/macro/normalize.zig` (already created)

**Algorithm**:
```zig
fn normalizeObjectSpreads(ctx: *NormalizeContext, node: *ast.Node) !*ast.Node {
    switch (node.kind) {
        .object_literal => {
            // 1. Detect spread elements
            // 2. For each spread: get type information (requires type checker)
            // 3. Expand spread into explicit field accesses
            // 4. Generate new object literal with all fields explicit
        },
        // Recursively visit children...
    }
}
```

**Challenge**: Requires **type information** to know what fields to expand

**Solution**: Two approaches:

**Approach A: Type-Driven Expansion** (Recommended)
- Run type checker first (lightweight pass)
- Get type of spread object
- Expand based on known fields
- **Pro**: Correct, type-safe
- **Con**: Requires type checker integration

**Approach B: Lazy Expansion**
- Leave spread as-is during normalization
- Mark for expansion during ownership analysis
- Expand when type is known
- **Pro**: No type checker dependency
- **Con**: Ownership analysis sees spread (less optimal)

**Recommendation**: Approach A (type-driven)

**Steps**:
1. Add `getTypeOf()` helper to NormalizeContext
2. Query type checker for spread object type
3. If type is known record type â†’ expand fields
4. If type is unknown â†’ leave as-is (fallback)

**Expected Impact**: **30-50% of patterns normalized** (common in modern TS)

---

### Week 6: Pass 2 - Array Method Chain Fusion

**File**: `src/macro/normalize.zig`

**Algorithm**:
```zig
fn fuseArrayMethodChains(ctx: *NormalizeContext, node: *ast.Node) !*ast.Node {
    // 1. Detect pattern: array.map(...).filter(...).reduce(...)
    // 2. Extract lambdas and combine logic
    // 3. Generate single for-loop
    // 4. Handle special cases (map-only, filter-only, etc.)
}
```

**Pattern Matching**:
```zig
// Detect chain:
// CallExpr { callee: MemberExpr { object: CallExpr { ... }, property: "reduce" } }
//            â†‘ This is a chain!
```

**Fusion Rules**:
| Pattern | Fused Code |
|---------|------------|
| `.map(f)` | `for (i) { result[i] = f(arr[i]); }` |
| `.filter(f)` | `for (i) { if (f(arr[i])) result.push(arr[i]); }` |
| `.map(f).filter(g)` | `for (i) { const x = f(arr[i]); if (g(x)) result.push(x); }` |
| `.map(f).reduce(g, init)` | `var acc = init; for (i) { acc = g(acc, f(arr[i])); }` |

**Challenge**: **Lambda analysis** (what does each lambda do?)

**Solution**: Lambda AST inspection
- Extract parameter name (e.g., `x`)
- Extract body expression (e.g., `x * 2`)
- Substitute into fused loop

**Steps**:
1. Detect method call chain pattern
2. Extract lambdas from each method
3. Combine lambda logic into single loop
4. Generate for-loop AST
5. Replace chain with for-loop

**Expected Impact**: **20-30% of array operations optimized** (common in functional style)

---

### Week 6: Pass 3 - Closure Inlining

**File**: `src/macro/normalize.zig`

**Algorithm**:
```zig
fn inlineSimpleClosures(ctx: *NormalizeContext, node: *ast.Node) !*ast.Node {
    // 1. Detect arrow function (closure)
    // 2. Perform escape analysis (what variables are captured?)
    // 3. Generate closure context struct
    // 4. Rewrite arrow function to use explicit context
}
```

**Escape Analysis**:
```zig
fn findCapturedVars(closure: *ast.Node) ![][]const u8 {
    // Walk closure body
    // Find all variable references
    // Filter to variables NOT in closure scope
    // Return list of captured variable names
}
```

**Transformation**:
```zig
// Before: () => start++
// After:
// 1. Generate struct: struct Closure_Ctx { start: number; }
// 2. Allocate context: const ctx = new Closure_Ctx();
// 3. Move captured vars: ctx.start = start;
// 4. Rewrite closure: (ctx) => ctx.start++
```

**Challenge**: **Name generation** (avoid conflicts)

**Solution**: Use unique names
- Context struct: `Closure_<func_name>_<counter>_Context`
- Closure function: `Closure_<func_name>_<counter>_fn`

**Expected Impact**: **10-20% of closures inlined** (less common than spreads/arrays)

---

## Integration with Type Checker

**Type checker must provide**:
1. `getTypeOf(node: *ast.Node) -> ?*ast.Type`
2. `getFields(type: *ast.Type) -> []FieldInfo`

**Normalization must run**:
- **After** parsing (raw AST available)
- **Before or during** type checking (need type info for spreads)

**Proposed Flow**:
```
1. Parse â†’ Raw AST
2. Type Check (Pass 1) â†’ Build type table
3. Normalize (with type info) â†’ Clean AST
4. Type Check (Pass 2) â†’ Type normalized AST
5. Ownership Analysis â†’ wants/provides
6. Codegen â†’ C/JS/Erlang
```

**Alternative** (simpler):
```
1. Parse â†’ Raw AST
2. Normalize (best-effort, without types) â†’ Partially clean AST
3. Type Check â†’ Typed AST
4. Ownership Analysis (with types) â†’ wants/provides + late normalization
5. Codegen â†’ C/JS/Erlang
```

**Recommendation**: Alternative (simpler, no circular dependency)

---

## Testing Strategy

### Unit Tests (Per Pass)

**Test normalization in isolation**:

```zig
test "normalize object spread - two objects" {
    // Input AST: const x = { ...obj1, ...obj2 };
    const input = createObjectSpreadAST(arena, "obj1", "obj2");

    // Run normalization
    const output = try normalizeObjectSpreads(&ctx, input);

    // Assert: output has explicit field accesses
    try testing.expect(output.kind == .object_literal);
    try testing.expect(countSpreadElements(output) == 0); // No spreads left
}
```

### Integration Tests (With Type Checker)

**Test with type information**:

```zig
test "normalize with types - spread User object" {
    // Given: class User { name: string; age: number; }
    const user_type = createClassType("User", &[_]Field{
        .{ .name = "name", .type = "string" },
        .{ .name = "age", .type = "number" },
    });

    // Input: const x = { ...user, extra: 42 };
    const input = createSpreadWithExtra();

    // Run normalization (with type info)
    const output = try normalizeObjectSpreads(&ctx, input);

    // Assert: 3 fields (name, age, extra)
    try testing.expectEqual(@as(usize, 3), countFields(output));
}
```

### Benchmark Tests

**Measure RC elimination improvement**:

```zig
test "benchmark: array chain fusion impact" {
    // Baseline: const result = items.map(x => x * 2).filter(x => x > 10);
    const baseline_rc_ops = measureRCOps(original_ast);

    // After normalization:
    const normalized = try fuseArrayMethodChains(&ctx, original_ast);
    const optimized_rc_ops = measureRCOps(normalized);

    // Assert: Significant reduction
    const reduction = @as(f64, @floatFromInt(baseline_rc_ops - optimized_rc_ops)) /
                      @as(f64, @floatFromInt(baseline_rc_ops));
    try testing.expect(reduction > 0.80); // >80% reduction
}
```

---

## Expected Impact (Validated by Research)

| Pass | Patterns Affected | RC Elimination | Impact |
|------|-------------------|----------------|--------|
| **Object Spread** | 30-50% of allocations | 90-100% (explicit fields) | **HIGH** |
| **Array Chains** | 20-30% of allocations | 100% (no intermediates) | **HIGH** |
| **Closures** | 10-20% of allocations | 50-80% (explicit context) | **MEDIUM** |

**Combined Impact**: **60-70% of dynamic patterns normalized**

**Enables Lobster Analysis**: With clean AST, Lobster ownership achieves **85-90% RC elimination**

**Final Overhead**: **0.5-1.5%** (validated by Nim ORC + Lobster research)

---

## Success Criteria (Week 6 Milestone)

### Functional Requirements
- âœ… All 3 passes implemented
- âœ… Unit tests passing (>80% coverage)
- âœ… Integration with type checker working
- âœ… Stats reporting (how many patterns normalized)

### Performance Requirements
- âœ… Normalization adds <10% to compile time
- âœ… At least 50% of target patterns normalized
- âœ… No crashes on complex codebases

### Quality Requirements
- âœ… Generated AST is well-formed (type checker accepts it)
- âœ… Source locations preserved (debugging works)
- âœ… Error messages still point to original source

---

## Next Steps (This Week)

### Day 1-2: Object Spread Implementation
1. Add type info queries to NormalizeContext
2. Implement field expansion logic
3. Write unit tests
4. Test on real TypeScript examples

### Day 3-4: Array Chain Fusion
1. Implement chain detection
2. Extract and combine lambdas
3. Generate for-loop AST
4. Write tests

### Day 5: Closure Inlining
1. Implement escape analysis
2. Generate context struct
3. Rewrite closure
4. Write tests

### Day 6-7: Integration & Testing
1. Integrate with compilation pipeline
2. Run on examples/\*.ms
3. Measure impact (stats reporting)
4. Document learnings

---

## References

**Validation Sources**:
- Lobster: 95% RC elimination (acyclic code) - [aardappel.github.io/lobster](https://aardappel.github.io/lobster/memory_management.html)
- V8: 90%+ short-lived allocations - v8.dev/blog/trash-talk
- ISMM 2012: 85-90% hybrid RC - Chang et al.

**Related Documents**:
- `docs/memory-model.md` - Full memory model design (validated)
- `docs/testing-infrastructure.md` - Benchmark framework
- `src/macro/normalize.zig` - Implementation

---

**Status**: Architecture complete, ready for implementation ðŸš€
